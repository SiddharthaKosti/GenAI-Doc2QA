{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\User\\Downloads\\sid_kosti\\GenAI\\iNeuron\\GenAI-Doc2QA\\data\\Siddhartha_Kosti_LLM_Resume.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for doc in document:\n",
    "    question_gen += doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(item) for item in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-4o-mini',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "Only create 5 questions.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "[Type text]  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Projects  \n",
      " \n",
      " LLM RAG Chatbot for Hospital system  \n",
      " Technologies used: LangChain, Neo4j AuraDB , Agents, \n",
      "Docker, Prompt Engineering , Streamlit , Litellm . \n",
      " Integrated LangChain and Neo4j AuraDB for data \n",
      "processing and storage . \n",
      " Developed Agents and customized prompts for text -to-\n",
      "SQL functionality . \n",
      " Dockerized the application for scalability and \n",
      "deployment.  \n",
      " Utilized litellm and groq -API for cost elimination  \n",
      " Designed a user interfac e with Streamlit . \n",
      " \n",
      " LLM Pipeline  evaluation  tests  \n",
      " Hallucination and Faithfulness test  \n",
      " Contextual relevancy test  \n",
      " Readability & Difficulty level test  \n",
      " Refusal and False Refusal test  \n",
      " \n",
      " Golden data set & Automated  Prompt  pipeline  \n",
      " Build  golden dataset for LLM pipeline benchmarking . \n",
      " Implementing  automated pipeline for prompt updates . \n",
      " \n",
      " RAG using llama2,  langchain  & streamlit  \n",
      " Streamlit  Interactive dashboard for Q&A tasks with \n",
      "upload  functionality on user side . \n",
      " \n",
      " Model  training on custom dataset  \n",
      " Utilized transfer learning and fine -tuning to enhance \n",
      "performance for client applications . \n",
      " Used Active Learning approach to obtain similar \n",
      "performance with smaller dataset . \n",
      " Examined TP, FP and FN , Failure Model analysis . \n",
      " Conducted Failure Mode Analysis to compared t wo \n",
      "different models performance . \n",
      " \n",
      " Clustering, Outlier, Drift & Labelling  tests  \n",
      " Utilized K -Means to c luster  train+ test dataset  to find \n",
      "similar data points , outliers  and drift if exists . \n",
      " Utilized SAM  (Segment Anything Model)  to find \n",
      "labeling consistency issues . \n",
      " \n",
      " Streamlit dashboards for clients  PoC \n",
      " Custom data visualization components to provide \n",
      "intuitive user interfaces tailored to client requirements . \n",
      " Worked collaboratively with team members to \n",
      "integrate findings and refine design iterations . \n",
      " Experience  \n",
      " \n",
      " Data Analyst:  Raga AI Pvt Ltd   \n",
      "06/2023  - Present  \n",
      " Created llm -chatbot for customer specific data \n",
      "using OpenAI model . \n",
      " Developed  LLM as a Judge based tests . \n",
      " Build  interactive streamlit dashboards for \n",
      "customer PoC .  \n",
      " Data-conversion & sanity check pipeline \n",
      "creation and execution . \n",
      " \n",
      " Data Analyst:  Affy Informatics Pvt Ltd  \n",
      "10/2021  - 05/2023  \n",
      " Built a RAG chatbot specialized  in NFT \n",
      "blockchain data linked  with a specific contract \n",
      "address . \n",
      " Analysed  blockchain data using open -source \n",
      "platform . \n",
      " Designed customized dashboards for client use . \n",
      " \n",
      " Research Experience  \n",
      "06/2014  - 09/2021   \n",
      " Over 7 years of inst ructing AI/ML courses  at \n",
      "under graduate level. \n",
      " Used GenAI to extract  and summarize data from \n",
      "scientific articles  and reaserch purpose . \n",
      " \n",
      "Education  \n",
      " \n",
      " M.Tech: IIT Kanpur           07/2011 – 7/2013  \n",
      " B.E.: MITS Gwalior             07/2006 – 6/2010  \n",
      "Tools  \n",
      " \n",
      " LLM based   \n",
      " OpenAI , Huggingface , Langfuse , LangChain , \n",
      "Litellm   \n",
      " Deployment : AWS,  Docker  \n",
      " Tracking : Git, GitHub  \n",
      " Dashboard : streamlit  \n",
      " Other: TensorFlow , PyTorch  \n",
      " \n",
      "Achievements  \n",
      " \n",
      " 2 research grants  of 22.36 lakh  INR to work on \n",
      "Solar PV -T Panel . \n",
      " AIR 677 in GATE 2011   \n",
      "SIDDHARTHA KOSTI  \n",
      "Generative AI  \n",
      "Comput er Vision    +91-7770806111  \n",
      " siddharth. kosti@gmail.com   \n",
      "  \n",
      "\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "Only create 5 questions.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ques = ques_gen_chain.run(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(document_answer_gen, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = ques.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**',\n",
       " '',\n",
       " '2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**',\n",
       " '',\n",
       " '3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**',\n",
       " '',\n",
       " '4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**',\n",
       " '',\n",
       " '5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**',\n",
       " '2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**',\n",
       " '3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**',\n",
       " '4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**',\n",
       " '5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list = [question for question in ques_list if question!=\"\"]\n",
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**\n",
      "Answer:  The technologies used in the LLM RAG Chatbot for the hospital system include LangChain, Neo4j AuraDB, Docker, Prompt Engineering, Streamlit, and Litellm. \n",
      "\n",
      "1. **LangChain**: This technology was integrated for building applications that utilize language models. It helps in managing the flow of data and interactions between the user and the language model.\n",
      "\n",
      "2. **Neo4j AuraDB**: This is a graph database used for data storage and processing. It allows for efficient querying and management of complex relationships in the data, which is particularly useful in a hospital system where data can be interconnected.\n",
      "\n",
      "3. **Docker**: Docker was used to containerize the application, which enhances scalability and simplifies deployment. This means that the application can be easily deployed across different environments without compatibility issues.\n",
      "\n",
      "4. **Prompt Engineering**: This involves designing and customizing prompts for the chatbot to improve its interaction quality and effectiveness in understanding user queries.\n",
      "\n",
      "5. **Streamlit**: This framework was utilized to design the user interface, providing an interactive and user-friendly dashboard for users to interact with the chatbot.\n",
      "\n",
      "6. **Litellm**: This technology was used for cost elimination, likely by optimizing the use of language models in the application.\n",
      "\n",
      "Together, these technologies contributed to a robust system that efficiently processes and stores data while providing a scalable and user-friendly application for hospital staff and patients.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**\n",
      "Answer:  The different evaluation tests conducted for the LLM Pipeline are:\n",
      "\n",
      "1. **Hallucination and Faithfulness Test**: This test assesses whether the language model generates information that is accurate and reliable. Hallucination refers to the model producing false or misleading information, while faithfulness evaluates how well the generated content aligns with the source material. The significance of this test lies in ensuring that the model provides trustworthy outputs, which is crucial for applications where accuracy is paramount.\n",
      "\n",
      "2. **Contextual Relevancy Test**: This test measures how well the model's responses are relevant to the given context or input. It evaluates whether the generated text appropriately addresses the user's query or topic. The significance of this test is to ensure that the model maintains coherence and relevance in its responses, enhancing user experience and satisfaction.\n",
      "\n",
      "3. **Readability & Difficulty Level Test**: This test assesses the clarity and comprehensibility of the generated text, as well as its appropriateness for the target audience's reading level. The significance of this test is to ensure that the model produces outputs that are easily understandable and suitable for the intended users, which is important for effective communication.\n",
      "\n",
      "4. **Refusal and False Refusal Test**: This test evaluates the model's ability to appropriately refuse to answer questions when it lacks sufficient information or when the query is inappropriate. It also checks for instances where the model incorrectly refuses to provide an answer. The significance of this test is to ensure that the model behaves responsibly and does not provide misleading or incorrect information, which is essential for maintaining user trust.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**\n",
      "Answer:  Transfer learning is a machine learning technique where a model developed for a particular task is reused as the starting point for a model on a second task. This approach is particularly useful when the second task has limited data available. By leveraging the knowledge gained from the first task, the model can achieve better performance on the second task more quickly and efficiently.\n",
      "\n",
      "Fine-tuning is a specific type of transfer learning where a pre-trained model is further trained (or \"tuned\") on a new dataset. This involves adjusting the model's weights based on the new data, allowing it to adapt to the specific characteristics of the new task while retaining the general knowledge it learned from the original dataset.\n",
      "\n",
      "In the context of model training on a custom dataset, these approaches enhance performance for client applications by:\n",
      "\n",
      "1. **Reducing Training Time**: Since the model starts with pre-learned features, it requires less time to converge on the new task compared to training a model from scratch.\n",
      "\n",
      "2. **Improving Accuracy**: Fine-tuning allows the model to adjust to the nuances of the new dataset, which can lead to improved accuracy and performance metrics.\n",
      "\n",
      "3. **Leveraging Existing Knowledge**: By using a model that has already learned from a large dataset, the new model can benefit from the general patterns and features that are applicable to the new task, even if the new dataset is smaller.\n",
      "\n",
      "Overall, these techniques enable more effective and efficient model training, leading to better outcomes for client applications.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**\n",
      "Answer:  In the clustering, outlier, drift, and labeling tests, K-Means and SAM (Segment Anything Model) were utilized to identify labeling consistency issues.\n",
      "\n",
      "1. **K-Means**: This method was used to cluster the training and test datasets to find similar data points, outliers, and any drift that may exist. By grouping data points into clusters, K-Means helps in identifying patterns and anomalies within the data, which can indicate inconsistencies in labeling.\n",
      "\n",
      "2. **SAM (Segment Anything Model)**: SAM was employed to find labeling consistency issues specifically. It assists in evaluating the quality and consistency of the labels assigned to the data points by segmenting the data and analyzing the labeling across different segments. This helps in identifying any discrepancies or inconsistencies in the labeling process.\n",
      "\n",
      "Together, K-Means and SAM provide a comprehensive approach to assessing the quality of data labeling, ensuring that the labels are consistent and reliable for further analysis and model training.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**\n",
      "Answer:  As a Data Analyst at Raga AI Pvt Ltd, I performed the following specific tasks related to the LLM chatbot and interactive Streamlit dashboards:\n",
      "\n",
      "1. **Created LLM-chatbot for customer-specific data**: I developed a chatbot using an OpenAI model tailored to handle and respond to queries based on specific customer data. This involved integrating the chatbot with the relevant datasets to ensure accurate and relevant responses.\n",
      "\n",
      "2. **Developed LLM as a Judge based tests**: I implemented tests that utilized the LLM to evaluate and judge responses or data quality, ensuring that the chatbot provided reliable information.\n",
      "\n",
      "3. **Built interactive Streamlit dashboards for customer PoC**: I designed and developed interactive dashboards using Streamlit, which allowed customers to visualize and interact with their data effectively. These dashboards were customized to meet specific client requirements, enhancing user experience and data accessibility.\n",
      "\n",
      "These tasks contributed to customer-specific data handling by ensuring that the chatbot was capable of providing tailored responses based on individual customer datasets, thereby improving the relevance and accuracy of interactions. The interactive dashboards facilitated better data visualization and decision-making for clients, allowing them to derive insights from their data more effectively.\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
