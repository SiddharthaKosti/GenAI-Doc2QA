{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Downloads\\\\sid_kosti\\\\GenAI\\\\iNeuron\\\\GenAI-Doc2QA\\\\research'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Downloads\\sid_kosti\\GenAI\\iNeuron\\GenAI-Doc2QA\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\User\\\\Downloads\\\\sid_kosti\\\\GenAI\\\\iNeuron\\\\GenAI-Doc2QA'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"data\\Siddhartha_Kosti_LLM_Resume.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for doc in document:\n",
    "    question_gen += doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(item) for item in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-4o-mini',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='[Type text]  \\n \\n \\n \\n \\n \\nProjects  \\n \\n\\uf0a7 LLM RAG Chatbot for Hospital system  \\n\\uf073 Technologies used: LangChain, Neo4j AuraDB , Agents, \\nDocker, Prompt Engineering , Streamlit , Litellm . \\n\\uf073 Integrated LangChain and Neo4j AuraDB for data \\nprocessing and storage . \\n\\uf073 Developed Agents and customized prompts for text -to-\\nSQL functionality . \\n\\uf073 Dockerized the application for scalability and \\ndeployment.  \\n\\uf073 Utilized litellm and groq -API for cost elimination  \\n\\uf073 Designed a user interfac e with Streamlit . \\n \\n\\uf0a7 LLM Pipeline  evaluation  tests  \\n\\uf073 Hallucination and Faithfulness test  \\n\\uf073 Contextual relevancy test  \\n\\uf073 Readability & Difficulty level test  \\n\\uf073 Refusal and False Refusal test  \\n \\n\\uf0a7 Golden data set & Automated  Prompt  pipeline  \\n\\uf073 Build  golden dataset for LLM pipeline benchmarking . \\n\\uf073 Implementing  automated pipeline for prompt updates . \\n \\n\\uf0a7 RAG using llama2,  langchain  & streamlit  \\n\\uf073 Streamlit  Interactive dashboard for Q&A tasks with \\nupload  functionality on user side . \\n \\n\\uf0a7 Model  training on custom dataset  \\n\\uf073 Utilized transfer learning and fine -tuning to enhance \\nperformance for client applications . \\n\\uf073 Used Active Learning approach to obtain similar \\nperformance with smaller dataset . \\n\\uf073 Examined TP, FP and FN , Failure Model analysis . \\n\\uf073 Conducted Failure Mode Analysis to compared t wo \\ndifferent models performance . \\n \\n\\uf0a7 Clustering, Outlier, Drift & Labelling  tests  \\n\\uf073 Utilized K -Means to c luster  train+ test dataset  to find \\nsimilar data points , outliers  and drift if exists . \\n\\uf073 Utilized SAM  (Segment Anything Model)  to find \\nlabeling consistency issues . \\n \\n\\uf0a7 Streamlit dashboards for clients  PoC \\n\\uf073 Custom data visualization components to provide \\nintuitive user interfaces tailored to client requirements . \\n\\uf073 Worked collaboratively with team members to \\nintegrate findings and refine design iterations . \\n Experience  \\n \\n\\uf0a7 Data Analyst:  Raga AI Pvt Ltd   \\n06/2023  - Present  \\n\\uf073 Created llm -chatbot for customer specific data \\nusing OpenAI model . \\n\\uf073 Developed  LLM as a Judge based tests . \\n\\uf073 Build  interactive streamlit dashboards for \\ncustomer PoC .  \\n\\uf073 Data-conversion & sanity check pipeline \\ncreation and execution . \\n \\n\\uf0a7 Data Analyst:  Affy Informatics Pvt Ltd  \\n10/2021  - 05/2023  \\n\\uf073 Built a RAG chatbot specialized  in NFT \\nblockchain data linked  with a specific contract \\naddress . \\n\\uf073 Analysed  blockchain data using open -source \\nplatform . \\n\\uf073 Designed customized dashboards for client use . \\n \\n\\uf0a7 Research Experience  \\n06/2014  - 09/2021   \\n\\uf073 Over 7 years of inst ructing AI/ML courses  at \\nunder graduate level. \\n\\uf073 Used GenAI to extract  and summarize data from \\nscientific articles  and reaserch purpose . \\n \\nEducation  \\n \\n\\uf0a7 M.Tech: IIT Kanpur           07/2011 – 7/2013  \\n\\uf0a7 B.E.: MITS Gwalior             07/2006 – 6/2010  \\nTools  \\n \\n\\uf0a7 LLM based   \\n\\uf073 OpenAI , Huggingface , Langfuse , LangChain , \\nLitellm   \\n\\uf0a7 Deployment : AWS,  Docker  \\n\\uf0a7 Tracking : Git, GitHub  \\n\\uf0a7 Dashboard : streamlit  \\n\\uf0a7 Other: TensorFlow , PyTorch  \\n \\nAchievements  \\n \\n\\uf073 2 research grants  of 22.36 lakh  INR to work on \\nSolar PV -T Panel . \\n\\uf073 AIR 677 in GATE 2011   \\nSIDDHARTHA KOSTI  \\nGenerative AI  \\nComput er Vision    +91-7770806111  \\n siddharth. kosti@gmail.com   \\n  \\n')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_answer_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "Only create 5 questions.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "[Type text]  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Projects  \n",
      " \n",
      " LLM RAG Chatbot for Hospital system  \n",
      " Technologies used: LangChain, Neo4j AuraDB , Agents, \n",
      "Docker, Prompt Engineering , Streamlit , Litellm . \n",
      " Integrated LangChain and Neo4j AuraDB for data \n",
      "processing and storage . \n",
      " Developed Agents and customized prompts for text -to-\n",
      "SQL functionality . \n",
      " Dockerized the application for scalability and \n",
      "deployment.  \n",
      " Utilized litellm and groq -API for cost elimination  \n",
      " Designed a user interfac e with Streamlit . \n",
      " \n",
      " LLM Pipeline  evaluation  tests  \n",
      " Hallucination and Faithfulness test  \n",
      " Contextual relevancy test  \n",
      " Readability & Difficulty level test  \n",
      " Refusal and False Refusal test  \n",
      " \n",
      " Golden data set & Automated  Prompt  pipeline  \n",
      " Build  golden dataset for LLM pipeline benchmarking . \n",
      " Implementing  automated pipeline for prompt updates . \n",
      " \n",
      " RAG using llama2,  langchain  & streamlit  \n",
      " Streamlit  Interactive dashboard for Q&A tasks with \n",
      "upload  functionality on user side . \n",
      " \n",
      " Model  training on custom dataset  \n",
      " Utilized transfer learning and fine -tuning to enhance \n",
      "performance for client applications . \n",
      " Used Active Learning approach to obtain similar \n",
      "performance with smaller dataset . \n",
      " Examined TP, FP and FN , Failure Model analysis . \n",
      " Conducted Failure Mode Analysis to compared t wo \n",
      "different models performance . \n",
      " \n",
      " Clustering, Outlier, Drift & Labelling  tests  \n",
      " Utilized K -Means to c luster  train+ test dataset  to find \n",
      "similar data points , outliers  and drift if exists . \n",
      " Utilized SAM  (Segment Anything Model)  to find \n",
      "labeling consistency issues . \n",
      " \n",
      " Streamlit dashboards for clients  PoC \n",
      " Custom data visualization components to provide \n",
      "intuitive user interfaces tailored to client requirements . \n",
      " Worked collaboratively with team members to \n",
      "integrate findings and refine design iterations . \n",
      " Experience  \n",
      " \n",
      " Data Analyst:  Raga AI Pvt Ltd   \n",
      "06/2023  - Present  \n",
      " Created llm -chatbot for customer specific data \n",
      "using OpenAI model . \n",
      " Developed  LLM as a Judge based tests . \n",
      " Build  interactive streamlit dashboards for \n",
      "customer PoC .  \n",
      " Data-conversion & sanity check pipeline \n",
      "creation and execution . \n",
      " \n",
      " Data Analyst:  Affy Informatics Pvt Ltd  \n",
      "10/2021  - 05/2023  \n",
      " Built a RAG chatbot specialized  in NFT \n",
      "blockchain data linked  with a specific contract \n",
      "address . \n",
      " Analysed  blockchain data using open -source \n",
      "platform . \n",
      " Designed customized dashboards for client use . \n",
      " \n",
      " Research Experience  \n",
      "06/2014  - 09/2021   \n",
      " Over 7 years of inst ructing AI/ML courses  at \n",
      "under graduate level. \n",
      " Used GenAI to extract  and summarize data from \n",
      "scientific articles  and reaserch purpose . \n",
      " \n",
      "Education  \n",
      " \n",
      " M.Tech: IIT Kanpur           07/2011 – 7/2013  \n",
      " B.E.: MITS Gwalior             07/2006 – 6/2010  \n",
      "Tools  \n",
      " \n",
      " LLM based   \n",
      " OpenAI , Huggingface , Langfuse , LangChain , \n",
      "Litellm   \n",
      " Deployment : AWS,  Docker  \n",
      " Tracking : Git, GitHub  \n",
      " Dashboard : streamlit  \n",
      " Other: TensorFlow , PyTorch  \n",
      " \n",
      "Achievements  \n",
      " \n",
      " 2 research grants  of 22.36 lakh  INR to work on \n",
      "Solar PV -T Panel . \n",
      " AIR 677 in GATE 2011   \n",
      "SIDDHARTHA KOSTI  \n",
      "Generative AI  \n",
      "Comput er Vision    +91-7770806111  \n",
      " siddharth. kosti@gmail.com   \n",
      "  \n",
      "\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "Only create 5 questions.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ques = ques_gen_chain.run(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(document_answer_gen, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = ques.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**',\n",
       " '',\n",
       " '2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**',\n",
       " '',\n",
       " '3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**',\n",
       " '',\n",
       " '4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**',\n",
       " '',\n",
       " '5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**',\n",
       " '2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**',\n",
       " '3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**',\n",
       " '4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**',\n",
       " '5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list = [question for question in ques_list if question!=\"\"]\n",
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. **Describe the technologies used in the LLM RAG Chatbot for the hospital system. How did these technologies contribute to data processing, storage, and application deployment?**\n",
      "Answer:  The technologies used in the LLM RAG Chatbot for the hospital system include LangChain, Neo4j AuraDB, Docker, Prompt Engineering, Streamlit, and Litellm. \n",
      "\n",
      "1. **LangChain**: This technology was integrated for building applications that utilize language models. It helps in managing the flow of data and interactions between the user and the language model.\n",
      "\n",
      "2. **Neo4j AuraDB**: This is a graph database used for data storage and processing. It allows for efficient querying and management of complex relationships in the data, which is particularly useful in a hospital system where data can be interconnected.\n",
      "\n",
      "3. **Docker**: Docker was used to containerize the application, which enhances scalability and simplifies deployment. This means that the application can be easily deployed across different environments without compatibility issues.\n",
      "\n",
      "4. **Prompt Engineering**: This involves designing and customizing prompts for the chatbot to improve its interaction quality and effectiveness in understanding user queries.\n",
      "\n",
      "5. **Streamlit**: This framework was utilized to design the user interface, providing an interactive and user-friendly dashboard for users to interact with the chatbot.\n",
      "\n",
      "6. **Litellm**: This technology was used for cost elimination, likely by optimizing the use of language models in the application.\n",
      "\n",
      "Together, these technologies contributed to a robust system that efficiently processes and stores data while providing a scalable and user-friendly application for hospital staff and patients.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  2. **What are the different evaluation tests conducted for the LLM Pipeline? Explain the significance of each test in assessing the performance of the language model.**\n",
      "Answer:  The different evaluation tests conducted for the LLM Pipeline are:\n",
      "\n",
      "1. **Hallucination and Faithfulness Test**: This test assesses whether the language model generates information that is accurate and reliable. Hallucination refers to the model producing false or misleading information, while faithfulness evaluates how well the generated content aligns with the source material. The significance of this test lies in ensuring that the model provides trustworthy outputs, which is crucial for applications where accuracy is paramount.\n",
      "\n",
      "2. **Contextual Relevancy Test**: This test measures how well the model's responses are relevant to the given context or input. It evaluates whether the generated text appropriately addresses the user's query or topic. The significance of this test is to ensure that the model maintains coherence and relevance in its responses, enhancing user experience and satisfaction.\n",
      "\n",
      "3. **Readability & Difficulty Level Test**: This test assesses the clarity and comprehensibility of the generated text, as well as its appropriateness for the target audience's reading level. The significance of this test is to ensure that the model produces outputs that are easily understandable and suitable for the intended users, which is important for effective communication.\n",
      "\n",
      "4. **Refusal and False Refusal Test**: This test evaluates the model's ability to appropriately refuse to answer questions when it lacks sufficient information or when the query is inappropriate. It also checks for instances where the model incorrectly refuses to provide an answer. The significance of this test is to ensure that the model behaves responsibly and does not provide misleading or incorrect information, which is essential for maintaining user trust.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  3. **In the context of model training on a custom dataset, explain the concepts of transfer learning and fine-tuning. How did these approaches enhance performance for client applications?**\n",
      "Answer:  Transfer learning is a machine learning technique where a model developed for a particular task is reused as the starting point for a model on a second task. This approach is particularly useful when the second task has limited data available. By leveraging the knowledge gained from the first task, the model can achieve better performance on the second task more quickly and efficiently.\n",
      "\n",
      "Fine-tuning is a specific type of transfer learning where a pre-trained model is further trained (or \"tuned\") on a new dataset. This involves adjusting the model's weights based on the new data, allowing it to adapt to the specific characteristics of the new task while retaining the general knowledge it learned from the original dataset.\n",
      "\n",
      "In the context of model training on a custom dataset, these approaches enhance performance for client applications by:\n",
      "\n",
      "1. **Reducing Training Time**: Since the model starts with pre-learned features, it requires less time to converge on the new task compared to training a model from scratch.\n",
      "\n",
      "2. **Improving Accuracy**: Fine-tuning allows the model to adjust to the nuances of the new dataset, which can lead to improved accuracy and performance metrics.\n",
      "\n",
      "3. **Leveraging Existing Knowledge**: By using a model that has already learned from a large dataset, the new model can benefit from the general patterns and features that are applicable to the new task, even if the new dataset is smaller.\n",
      "\n",
      "Overall, these techniques enable more effective and efficient model training, leading to better outcomes for client applications.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  4. **What methods were utilized to identify labeling consistency issues in the clustering, outlier, drift, and labeling tests? Discuss the role of K-Means and SAM (Segment Anything Model) in this process.**\n",
      "Answer:  In the clustering, outlier, drift, and labeling tests, K-Means and SAM (Segment Anything Model) were utilized to identify labeling consistency issues.\n",
      "\n",
      "1. **K-Means**: This method was used to cluster the training and test datasets to find similar data points, outliers, and any drift that may exist. By grouping data points into clusters, K-Means helps in identifying patterns and anomalies within the data, which can indicate inconsistencies in labeling.\n",
      "\n",
      "2. **SAM (Segment Anything Model)**: SAM was employed to find labeling consistency issues specifically. It assists in evaluating the quality and consistency of the labels assigned to the data points by segmenting the data and analyzing the labeling across different segments. This helps in identifying any discrepancies or inconsistencies in the labeling process.\n",
      "\n",
      "Together, K-Means and SAM provide a comprehensive approach to assessing the quality of data labeling, ensuring that the labels are consistent and reliable for further analysis and model training.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  5. **As a Data Analyst at Raga AI Pvt Ltd, what specific tasks did you perform related to the LLM chatbot and interactive Streamlit dashboards? How did these tasks contribute to customer-specific data handling?**\n",
      "Answer:  As a Data Analyst at Raga AI Pvt Ltd, I performed the following specific tasks related to the LLM chatbot and interactive Streamlit dashboards:\n",
      "\n",
      "1. **Created LLM-chatbot for customer-specific data**: I developed a chatbot using an OpenAI model tailored to handle and respond to queries based on specific customer data. This involved integrating the chatbot with the relevant datasets to ensure accurate and relevant responses.\n",
      "\n",
      "2. **Developed LLM as a Judge based tests**: I implemented tests that utilized the LLM to evaluate and judge responses or data quality, ensuring that the chatbot provided reliable information.\n",
      "\n",
      "3. **Built interactive Streamlit dashboards for customer PoC**: I designed and developed interactive dashboards using Streamlit, which allowed customers to visualize and interact with their data effectively. These dashboards were customized to meet specific client requirements, enhancing user experience and data accessibility.\n",
      "\n",
      "These tasks contributed to customer-specific data handling by ensuring that the chatbot was capable of providing tailored responses based on individual customer datasets, thereby improving the relevance and accuracy of interactions. The interactive dashboards facilitated better data visualization and decision-making for clients, allowing them to derive insights from their data more effectively.\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import llm_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\User\\Downloads\\sid_kosti\\GenAI\\iNeuron\\GenAI-Doc2QA\\data\\Siddhartha_Kosti_LLM_Resume.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "[Type text]  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Projects  \n",
      " \n",
      " LLM RAG Chatbot for Hospital system  \n",
      " Technologies used: LangChain, Neo4j AuraDB , Agents, \n",
      "Docker, Prompt Engineering , Streamlit , Litellm . \n",
      " Integrated LangChain and Neo4j AuraDB for data \n",
      "processing and storage . \n",
      " Developed Agents and customized prompts for text -to-\n",
      "SQL functionality . \n",
      " Dockerized the application for scalability and \n",
      "deployment.  \n",
      " Utilized litellm and groq -API for cost elimination  \n",
      " Designed a user interfac e with Streamlit . \n",
      " \n",
      " LLM Pipeline  evaluation  tests  \n",
      " Hallucination and Faithfulness test  \n",
      " Contextual relevancy test  \n",
      " Readability & Difficulty level test  \n",
      " Refusal and False Refusal test  \n",
      " \n",
      " Golden data set & Automated  Prompt  pipeline  \n",
      " Build  golden dataset for LLM pipeline benchmarking . \n",
      " Implementing  automated pipeline for prompt updates . \n",
      " \n",
      " RAG using llama2,  langchain  & streamlit  \n",
      " Streamlit  Interactive dashboard for Q&A tasks with \n",
      "upload  functionality on user side . \n",
      " \n",
      " Model  training on custom dataset  \n",
      " Utilized transfer learning and fine -tuning to enhance \n",
      "performance for client applications . \n",
      " Used Active Learning approach to obtain similar \n",
      "performance with smaller dataset . \n",
      " Examined TP, FP and FN , Failure Model analysis . \n",
      " Conducted Failure Mode Analysis to compared t wo \n",
      "different models performance . \n",
      " \n",
      " Clustering, Outlier, Drift & Labelling  tests  \n",
      " Utilized K -Means to c luster  train+ test dataset  to find \n",
      "similar data points , outliers  and drift if exists . \n",
      " Utilized SAM  (Segment Anything Model)  to find \n",
      "labeling consistency issues . \n",
      " \n",
      " Streamlit dashboards for clients  PoC \n",
      " Custom data visualization components to provide \n",
      "intuitive user interfaces tailored to client requirements . \n",
      " Worked collaboratively with team members to \n",
      "integrate findings and refine design iterations . \n",
      " Experience  \n",
      " \n",
      " Data Analyst:  Raga AI Pvt Ltd   \n",
      "06/2023  - Present  \n",
      " Created llm -chatbot for customer specific data \n",
      "using OpenAI model . \n",
      " Developed  LLM as a Judge based tests . \n",
      " Build  interactive streamlit dashboards for \n",
      "customer PoC .  \n",
      " Data-conversion & sanity check pipeline \n",
      "creation and execution . \n",
      " \n",
      " Data Analyst:  Affy Informatics Pvt Ltd  \n",
      "10/2021  - 05/2023  \n",
      " Built a RAG chatbot specialized  in NFT \n",
      "blockchain data linked  with a specific contract \n",
      "address . \n",
      " Analysed  blockchain data using open -source \n",
      "platform . \n",
      " Designed customized dashboards for client use . \n",
      " \n",
      " Research Experience  \n",
      "06/2014  - 09/2021   \n",
      " Over 7 years of inst ructing AI/ML courses  at \n",
      "under graduate level. \n",
      " Used GenAI to extract  and summarize data from \n",
      "scientific articles  and reaserch purpose . \n",
      " \n",
      "Education  \n",
      " \n",
      " M.Tech: IIT Kanpur           07/2011 – 7/2013  \n",
      " B.E.: MITS Gwalior             07/2006 – 6/2010  \n",
      "Tools  \n",
      " \n",
      " LLM based   \n",
      " OpenAI , Huggingface , Langfuse , LangChain , \n",
      "Litellm   \n",
      " Deployment : AWS,  Docker  \n",
      " Tracking : Git, GitHub  \n",
      " Dashboard : streamlit  \n",
      " Other: TensorFlow , PyTorch  \n",
      " \n",
      "Achievements  \n",
      " \n",
      " 2 research grants  of 22.36 lakh  INR to work on \n",
      "Solar PV -T Panel . \n",
      " AIR 677 in GATE 2011   \n",
      "SIDDHARTHA KOSTI  \n",
      "Generative AI  \n",
      "Comput er Vision    +91-7770806111  \n",
      " siddharth. kosti@gmail.com   \n",
      "  \n",
      "\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "Only create 5 questions.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer_generation_chain, ques_list = llm_pipeline(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. **Technologies and Integration**: Describe how you integrated LangChain and Neo4j AuraDB in the LLM RAG Chatbot project. What role did each technology play in data processing and storage?\n",
      "Answer:  In the LLM RAG Chatbot project, LangChain was utilized for building the language model capabilities and facilitating interactions with the data, while Neo4j AuraDB served as the database for data storage and processing. \n",
      "\n",
      "LangChain helped in creating the framework for the chatbot, enabling it to process natural language queries and generate responses based on the data stored in the database. It also facilitated the development of agents and customized prompts for text-to-SQL functionality, allowing the chatbot to convert user queries into SQL queries to retrieve relevant data.\n",
      "\n",
      "On the other hand, Neo4j AuraDB was used for storing the data in a graph database format, which is particularly effective for handling complex relationships and connections within the data. This integration allowed for efficient data retrieval and processing, enabling the chatbot to provide accurate and contextually relevant responses to user queries.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  2. **Model Evaluation**: What are the key tests conducted in the LLM Pipeline evaluation phase? Explain the significance of the Hallucination and Faithfulness test in evaluating language models.\n",
      "Answer:  The key tests conducted in the LLM Pipeline evaluation phase include:\n",
      "\n",
      "1. Hallucination and Faithfulness test\n",
      "2. Contextual relevancy test\n",
      "3. Readability & Difficulty level test\n",
      "4. Refusal and False Refusal test\n",
      "\n",
      "The significance of the Hallucination and Faithfulness test in evaluating language models lies in its ability to assess the accuracy and reliability of the model's outputs. \n",
      "\n",
      "- **Hallucination** refers to instances where the model generates information that is not grounded in the input data or factual reality. This can lead to the dissemination of false or misleading information, which is particularly critical in applications where accuracy is paramount, such as healthcare or legal contexts.\n",
      "\n",
      "- **Faithfulness** measures how well the generated content aligns with the source material or the input provided to the model. A faithful model should produce outputs that are true to the input context and do not introduce inaccuracies or fabrications.\n",
      "\n",
      "Together, these tests help ensure that language models provide trustworthy and relevant information, which is essential for user confidence and the practical application of these models in real-world scenarios.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  3. **Data Handling**: In the context of building a golden dataset for LLM pipeline benchmarking, what steps would you take to ensure the dataset is comprehensive and reliable? How does an automated prompt pipeline contribute to this process?\n",
      "Answer:  To ensure that the golden dataset for LLM pipeline benchmarking is comprehensive and reliable, the following steps can be taken:\n",
      "\n",
      "1. **Data Collection**: Gather a diverse set of data that covers various scenarios and edge cases relevant to the tasks the LLM will perform. This may include different formats, contexts, and types of input data.\n",
      "\n",
      "2. **Data Cleaning**: Remove any duplicates, irrelevant information, or inconsistencies in the dataset. This step is crucial to ensure that the data is accurate and of high quality.\n",
      "\n",
      "3. **Data Annotation**: If necessary, annotate the data to provide context or labels that will help in evaluating the LLM's performance. This could involve tagging parts of the data or providing additional metadata.\n",
      "\n",
      "4. **Validation**: Implement a validation process to check the reliability of the dataset. This could involve cross-referencing with existing datasets or expert reviews to ensure the data's accuracy.\n",
      "\n",
      "5. **Benchmarking**: Establish clear metrics and benchmarks that the LLM should meet when evaluated against the golden dataset. This will help in assessing the model's performance effectively.\n",
      "\n",
      "6. **Iterative Updates**: Regularly update the dataset based on new findings, user feedback, or changes in the domain to keep it relevant and comprehensive.\n",
      "\n",
      "The automated prompt pipeline contributes to this process by streamlining the process of updating and refining prompts used in the LLM. It allows for quick iterations and adjustments based on performance metrics, ensuring that the prompts remain effective and aligned with the goals of the benchmarking. This automation can help in maintaining the quality of the dataset and adapting it to new requirements or insights gained during the evaluation process.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  4. **Machine Learning Techniques**: Explain the concepts of transfer learning and fine-tuning as utilized in model training on a custom dataset. How does the Active Learning approach help in achieving similar performance with a smaller dataset?\n",
      "Answer:  Transfer learning is a machine learning technique where a model developed for a particular task is reused as the starting point for a model on a second task. This approach is particularly useful when the second task has a limited amount of data. By leveraging the knowledge gained from the first task, the model can achieve better performance on the second task more quickly and efficiently.\n",
      "\n",
      "Fine-tuning is a specific type of transfer learning where the pre-trained model is further trained (or \"tuned\") on the new dataset. During fine-tuning, the model's weights are adjusted based on the new data, allowing it to adapt to the specific characteristics of the custom dataset while retaining the general knowledge it acquired from the original training.\n",
      "\n",
      "Active Learning is an iterative approach to machine learning where the model identifies which data points would be most beneficial to learn from. In the context of achieving similar performance with a smaller dataset, Active Learning helps by selecting the most informative samples from a larger pool of unlabeled data. By focusing on these key samples, the model can improve its performance without needing to train on the entire dataset, thus reducing the amount of labeled data required and making the training process more efficient.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  5. **Dashboard Development**: What are the essential features of the Streamlit dashboards developed for client Proof of Concept (PoC)? How did you ensure that the dashboards met client requirements through collaboration with team members?\n",
      "Answer:  The essential features of the Streamlit dashboards developed for client Proof of Concept (PoC) include custom data visualization components tailored to client requirements. To ensure that the dashboards met client requirements, collaboration with team members was key. This involved integrating findings from various team members and refining design iterations based on feedback and discussions, ensuring that the final product aligned with client expectations and needs.\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
