{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dotenv\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\User\\Downloads\\sid_kosti\\GenAI\\iNeuron\\GenAI-Doc2QA\\data\\Siddhartha_Kosti_LLM_Resume.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_gen = \"\"\n",
    "for doc in document:\n",
    "    question_gen += doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ques_gen = TokenTextSplitter(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    chunk_size=10000,\n",
    "    chunk_overlap=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_ques_gen = splitter_ques_gen.split_text(question_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ques_gen = [Document(item) for item in chunk_ques_gen]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter_ans_gen = TokenTextSplitter(\n",
    "    model_name = 'gpt-4o-mini',\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_answer_gen = splitter_ans_gen.split_documents(\n",
    "    document_ques_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_ques_gen_pipeline = ChatOpenAI(\n",
    "    model = 'gpt-4o-mini',\n",
    "    temperature = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are an expert at creating questions based on coding materials and documentation.\n",
    "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
    "You do this by asking questions about the text below:\n",
    "\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Create questions that will prepare the coders or programmers for their tests.\n",
    "Make sure not to lose any important information.\n",
    "Only create 5 questions.\n",
    "\n",
    "QUESTIONS:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_QUESTIONS = PromptTemplate(template=prompt_template, input_variables=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_template = (\"\"\"\n",
    "You are an expert at creating practice questions based on coding material and documentation.\n",
    "Your goal is to help a coder or programmer prepare for a coding test.\n",
    "We have received some practice questions to a certain extent: {existing_answer}.\n",
    "We have the option to refine the existing questions or add new ones.\n",
    "(only if necessary) with some more context below.\n",
    "------------\n",
    "{text}\n",
    "------------\n",
    "\n",
    "Given the new context, refine the original questions in English.\n",
    "If the context is not helpful, please provide the original questions.\n",
    "QUESTIONS:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINE_PROMPT_QUESTIONS = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_gen_chain = load_summarize_chain(llm = llm_ques_gen_pipeline, \n",
    "                                          chain_type = \"refine\", \n",
    "                                          verbose = True, \n",
    "                                          question_prompt=PROMPT_QUESTIONS, \n",
    "                                          refine_prompt=REFINE_PROMPT_QUESTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are an expert at creating questions based on coding materials and documentation.\n",
      "Your goal is to prepare a coder or programmer for their exam and coding tests.\n",
      "You do this by asking questions about the text below:\n",
      "\n",
      "------------\n",
      "[Type text]  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Projects  \n",
      " \n",
      " LLM RAG Chatbot for Hospital system  \n",
      " Technologies used: LangChain, Neo4j AuraDB , Agents, \n",
      "Docker, Prompt Engineering , Streamlit , Litellm . \n",
      " Integrated LangChain and Neo4j AuraDB for data \n",
      "processing and storage . \n",
      " Developed Agents and customized prompts for text -to-\n",
      "SQL functionality . \n",
      " Dockerized the application for scalability and \n",
      "deployment.  \n",
      " Utilized litellm and groq -API for cost elimination  \n",
      " Designed a user interfac e with Streamlit . \n",
      " \n",
      " LLM Pipeline  evaluation  tests  \n",
      " Hallucination and Faithfulness test  \n",
      " Contextual relevancy test  \n",
      " Readability & Difficulty level test  \n",
      " Refusal and False Refusal test  \n",
      " \n",
      " Golden data set & Automated  Prompt  pipeline  \n",
      " Build  golden dataset for LLM pipeline benchmarking . \n",
      " Implementing  automated pipeline for prompt updates . \n",
      " \n",
      " RAG using llama2,  langchain  & streamlit  \n",
      " Streamlit  Interactive dashboard for Q&A tasks with \n",
      "upload  functionality on user side . \n",
      " \n",
      " Model  training on custom dataset  \n",
      " Utilized transfer learning and fine -tuning to enhance \n",
      "performance for client applications . \n",
      " Used Active Learning approach to obtain similar \n",
      "performance with smaller dataset . \n",
      " Examined TP, FP and FN , Failure Model analysis . \n",
      " Conducted Failure Mode Analysis to compared t wo \n",
      "different models performance . \n",
      " \n",
      " Clustering, Outlier, Drift & Labelling  tests  \n",
      " Utilized K -Means to c luster  train+ test dataset  to find \n",
      "similar data points , outliers  and drift if exists . \n",
      " Utilized SAM  (Segment Anything Model)  to find \n",
      "labeling consistency issues . \n",
      " \n",
      " Streamlit dashboards for clients  PoC \n",
      " Custom data visualization components to provide \n",
      "intuitive user interfaces tailored to client requirements . \n",
      " Worked collaboratively with team members to \n",
      "integrate findings and refine design iterations . \n",
      " Experience  \n",
      " \n",
      " Data Analyst:  Raga AI Pvt Ltd   \n",
      "06/2023  - Present  \n",
      " Created llm -chatbot for customer specific data \n",
      "using OpenAI model . \n",
      " Developed  LLM as a Judge based tests . \n",
      " Build  interactive streamlit dashboards for \n",
      "customer PoC .  \n",
      " Data-conversion & sanity check pipeline \n",
      "creation and execution . \n",
      " \n",
      " Data Analyst:  Affy Informatics Pvt Ltd  \n",
      "10/2021  - 05/2023  \n",
      " Built a RAG chatbot specialized  in NFT \n",
      "blockchain data linked  with a specific contract \n",
      "address . \n",
      " Analysed  blockchain data using open -source \n",
      "platform . \n",
      " Designed customized dashboards for client use . \n",
      " \n",
      " Research Experience  \n",
      "06/2014  - 09/2021   \n",
      " Over 7 years of inst ructing AI/ML courses  at \n",
      "under graduate level. \n",
      " Used GenAI to extract  and summarize data from \n",
      "scientific articles  and reaserch purpose . \n",
      " \n",
      "Education  \n",
      " \n",
      " M.Tech: IIT Kanpur           07/2011 – 7/2013  \n",
      " B.E.: MITS Gwalior             07/2006 – 6/2010  \n",
      "Tools  \n",
      " \n",
      " LLM based   \n",
      " OpenAI , Huggingface , Langfuse , LangChain , \n",
      "Litellm   \n",
      " Deployment : AWS,  Docker  \n",
      " Tracking : Git, GitHub  \n",
      " Dashboard : streamlit  \n",
      " Other: TensorFlow , PyTorch  \n",
      " \n",
      "Achievements  \n",
      " \n",
      " 2 research grants  of 22.36 lakh  INR to work on \n",
      "Solar PV -T Panel . \n",
      " AIR 677 in GATE 2011   \n",
      "SIDDHARTHA KOSTI  \n",
      "Generative AI  \n",
      "Comput er Vision    +91-7770806111  \n",
      " siddharth. kosti@gmail.com   \n",
      "  \n",
      "\n",
      "------------\n",
      "\n",
      "Create questions that will prepare the coders or programmers for their tests.\n",
      "Make sure not to lose any important information.\n",
      "Only create 5 questions.\n",
      "\n",
      "QUESTIONS:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ques = ques_gen_chain.run(document_ques_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\genai_qa\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(document_answer_gen, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_answer_gen = ChatOpenAI(temperature=0.1, model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_list = ques.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. **Technologies and Integration**: Describe how LangChain and Neo4j AuraDB were integrated in the LLM RAG Chatbot project. What role did each technology play in data processing and storage?',\n",
       " '',\n",
       " '2. **Testing Methodologies**: Outline the different evaluation tests conducted for the LLM Pipeline. What are the objectives of the Hallucination and Faithfulness test, and how do they contribute to the overall assessment of the model?',\n",
       " '',\n",
       " '3. **Model Training Techniques**: Explain the concepts of transfer learning and fine-tuning as utilized in model training on a custom dataset. How does the Active Learning approach help achieve similar performance with a smaller dataset?',\n",
       " '',\n",
       " '4. **Clustering and Analysis**: In the context of clustering, what is the purpose of using K-Means for the train and test dataset? Additionally, how does the Segment Anything Model (SAM) assist in identifying labeling consistency issues?',\n",
       " '',\n",
       " '5. **Dashboard Development**: What are the key features of the Streamlit dashboards developed for client Proof of Concept (PoC)? How did collaboration with team members influence the design iterations of these dashboards?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_chain = RetrievalQA.from_chain_type(llm=llm_answer_gen, \n",
    "                                               chain_type=\"stuff\", \n",
    "                                               retriever=vector_store.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  1. **Technologies and Integration**: Describe how LangChain and Neo4j AuraDB were integrated in the LLM RAG Chatbot project. What role did each technology play in data processing and storage?\n",
      "Answer:  In the LLM RAG Chatbot project, LangChain and Neo4j AuraDB were integrated to facilitate data processing and storage. \n",
      "\n",
      "- **LangChain**: This technology was utilized for building the chatbot's functionality, particularly in handling language models and managing the interactions between the user and the chatbot. It likely played a crucial role in processing user queries and generating responses based on the underlying data.\n",
      "\n",
      "- **Neo4j AuraDB**: This is a graph database service that was used for data storage. It allowed for efficient storage and retrieval of data, particularly in scenarios where relationships between data points are important. In the context of the chatbot, Neo4j AuraDB would have been used to store structured data that the chatbot could query to provide relevant answers to user inquiries.\n",
      "\n",
      "Together, these technologies enabled the chatbot to process natural language inputs and access a structured database to deliver accurate and contextually relevant responses.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  2. **Testing Methodologies**: Outline the different evaluation tests conducted for the LLM Pipeline. What are the objectives of the Hallucination and Faithfulness test, and how do they contribute to the overall assessment of the model?\n",
      "Answer:  The different evaluation tests conducted for the LLM Pipeline include:\n",
      "\n",
      "1. Hallucination and Faithfulness test\n",
      "2. Contextual relevancy test\n",
      "3. Readability & Difficulty level test\n",
      "4. Refusal and False Refusal test\n",
      "\n",
      "**Objectives of the Hallucination and Faithfulness test:**\n",
      "\n",
      "- **Hallucination Test**: This test aims to identify instances where the model generates information that is not grounded in the provided context or factual data. The objective is to ensure that the model does not produce misleading or false information that could misinform users.\n",
      "\n",
      "- **Faithfulness Test**: This test evaluates whether the model's outputs accurately reflect the input data or context. The goal is to assess the reliability of the model's responses and ensure that they are true to the source material.\n",
      "\n",
      "**Contribution to Overall Assessment of the Model**:\n",
      "\n",
      "Both tests contribute significantly to the overall assessment of the model by ensuring that the generated outputs are both accurate and trustworthy. By minimizing hallucinations and maximizing faithfulness, these tests help enhance the model's credibility and usability, which is crucial for applications where reliable information is essential, such as in healthcare or legal contexts.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  3. **Model Training Techniques**: Explain the concepts of transfer learning and fine-tuning as utilized in model training on a custom dataset. How does the Active Learning approach help achieve similar performance with a smaller dataset?\n",
      "Answer:  Transfer learning is a technique where a pre-trained model, which has been trained on a large dataset, is adapted to a new, often smaller dataset. This approach leverages the knowledge gained from the initial training to improve performance on the new task. In model training on a custom dataset, transfer learning allows for faster convergence and better performance, especially when the new dataset is limited in size.\n",
      "\n",
      "Fine-tuning is a specific form of transfer learning where the pre-trained model is further trained (or \"tuned\") on the new dataset. This involves adjusting the model's weights based on the new data, allowing it to learn task-specific features while retaining the general knowledge it acquired during the initial training. Fine-tuning typically requires fewer epochs and less data compared to training a model from scratch.\n",
      "\n",
      "Active Learning is an iterative approach where the model identifies which data points it is uncertain about and requests labels for those specific instances. This method helps achieve similar performance with a smaller dataset by focusing on the most informative samples, thereby maximizing the model's learning efficiency. By selectively querying labels for data points that are likely to improve the model's performance, Active Learning reduces the amount of labeled data needed while still achieving high accuracy.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  4. **Clustering and Analysis**: In the context of clustering, what is the purpose of using K-Means for the train and test dataset? Additionally, how does the Segment Anything Model (SAM) assist in identifying labeling consistency issues?\n",
      "Answer:  The purpose of using K-Means for the train and test dataset in clustering is to group similar data points together. This helps in identifying patterns, outliers, and any drift that may exist within the data. By clustering the data, analysts can gain insights into the structure of the dataset and understand how different data points relate to one another.\n",
      "\n",
      "The Segment Anything Model (SAM) assists in identifying labeling consistency issues by analyzing the labeled data and checking for inconsistencies in how data points are labeled. SAM can help ensure that similar data points are labeled in the same way, which is crucial for maintaining the quality and reliability of the dataset used for training machine learning models.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  \n",
      "Answer:  I don't know.\n",
      "--------------------------------------------------\\n\\n\n",
      "Question:  5. **Dashboard Development**: What are the key features of the Streamlit dashboards developed for client Proof of Concept (PoC)? How did collaboration with team members influence the design iterations of these dashboards?\n",
      "Answer:  The key features of the Streamlit dashboards developed for client Proof of Concept (PoC) include:\n",
      "\n",
      "- Custom data visualization components tailored to client requirements.\n",
      "- Interactive elements that enhance user engagement and experience.\n",
      "- User-friendly interfaces designed to present data intuitively.\n",
      "\n",
      "Collaboration with team members influenced the design iterations by allowing for the integration of findings and refinements based on collective feedback, which helped to improve the overall design and functionality of the dashboards.\n",
      "--------------------------------------------------\\n\\n\n"
     ]
    }
   ],
   "source": [
    "for question in ques_list:\n",
    "    print(\"Question: \", question)\n",
    "    answer = answer_generation_chain.run(question)\n",
    "    print(\"Answer: \", answer)\n",
    "    print(\"--------------------------------------------------\\\\n\\\\n\")\n",
    "    # Save answer to file\n",
    "    with open(\"answers.txt\", \"a\") as f:\n",
    "        f.write(\"Question: \" + question + \"\\\\n\")\n",
    "        f.write(\"Answer: \" + answer + \"\\\\n\")\n",
    "        f.write(\"--------------------------------------------------\\\\n\\\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
